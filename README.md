# SecurityEval + GPT Vulnerability Detection and Fixing Demo

This project demonstrates how large language models (LLMs) like GPT-3.5 can be used to detect and repair code vulnerabilities using examples from the SecurityEval dataset. It provides an interactive interface where users can load real-world vulnerable code prompts, run GPT-based vulnerability detection, suggest secure fixes, and validate results using static analysis tools like Bandit.

## Features

- Load prompts from the SecurityEval benchmark dataset

- Use GPT-3.5 to detect security vulnerabilities in Python code

- Automatically generate secure code replacements with explanations

- View the original vulnerable reference code for comparison

- Run Bandit static analysis to validate results

- Fully editable code interface using Streamlit Ace

- Easy-to-understand and professional web UI via Streamlit

## How It Works

1. The app loads a .jsonl dataset containing Prompt and Insecure_code entries.

3. The Prompt (partial or full function) is loaded into a code editor and used as the main input to GPT.

5. When the user clicks "Detect Vulnerabilities", a prompt is sent to GPT-3.5 asking it to analyze the code and return a list of vulnerabilities with CWE IDs and descriptions.

7. Clicking "Suggest Fix" sends the same code to GPT with instructions to fix the security issues and explain the changes.

9. Optionally, the user can run Bandit, a Python static analysis tool, on the current code to validate security issues statically.

11. The Insecure_code is shown for comparison only — it represents the vulnerable code generated by a previous LLM in the dataset.

### Setup Instructions

1. Clone the repository

```bash
git clone https://github.com/your-username/securityeval-demo.git
cd securityeval-demo
```

2. Install dependencies

```bash
pip install -r requirements.txt
```

3. Add your OpenAI API key

Use the .env file in the root folder:

OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

You can obtain an API key from https://platform.openai.com.

4. Run the application

```bash
streamlit run app.py
```

Visit http://localhost:8501 in your browser.

## Dataset Format

Each entry in data/securityeval.jsonl looks like this:

```json
{"ID": "CWE-798_demo", "Prompt": "def get_credentials():\n    pass", "Insecure_code": "def get_credentials():\n    return ('admin', '1234')"}
{"ID": "CWE-078_demo", "Prompt": "def run_command(cmd):\n    pass", "Insecure_code": "import os\ndef run_command(cmd):\n    os.system(cmd)"}
```

**Prompt:** The beginning of a function (what a developer might write or Copilot might start with)

**Insecure_code:** A known vulnerable completion generated by an earlier LLM

Only the Prompt is analyzed and repaired in the app

## Application Workflow

1. **Select Sample:** Choose a sample from the dropdown menu in the sidebar.

2. **Edit Code (Optional):** Modify the prompt code to test other scenarios.

3. **Run Vulnerability Detection:** Uses GPT-3.5 to return a list of CWE vulnerabilities with explanations.

4. ** Suggest Fix:** Uses GPT-3.5 to rewrite the code securely and explain the changes.

5. **Run Bandit:** Static analysis to find basic vulnerabilities in the current version of the code.

## Model Information

- **Model used:** gpt-3.5-turbo

- **Why:** It is the most affordable and widely available model in OpenAI’s API lineup, costing ~$0.0015 per 1K tokens.

- **How it's used:** The app uses the Chat API to simulate security auditors and code fixers with carefully constructed prompts.

## Requirements

```
streamlit
streamlit-ace
openai
python-dotenv
bandit
```

**Install using:**

```bash
pip install -r requirements.txt
```

## Tips

- You can add your own .jsonl files to data/ to test custom prompts.

- You can replace the GPT model with gpt-4 if you have API access by changing the model name in app.py.

- Use the editor to write fresh insecure functions and test GPT's ability to detect/fix them.

## Future Enhancements

- Side-by-side diff viewer for before/after

- Output quality scoring or rating

- Logging of all interactions for research analysis

- Export secure code to file

## License and Attribution

This project is for research and educational purposes. It is built upon the SecurityEval dataset by s2e-lab and uses OpenAI's GPT API for language model services.

## Maintainer

Developed and maintained by Mohamed Sakr. 
